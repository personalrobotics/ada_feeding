# Configuration file for the feeding demo with the Gen2 arm
#
# If not otherwise specified:
#     All units are metric.
#     All forces and torques are in N and Nm, respectively.
#     All distances are in m.
#     All rotations are in radians.
#     All robot configurations specify 6 joint angles in a list
#     All transform use the format [x,y,z,rx,ry,rz] of positions and rotations in a list.
#

af_named_configs: package://ada_feeding/config/named_configs_gen2_6dof.yaml

# Visualization parameters
visualization:
  topicName: dart_markers/feeding
  baseFrameName: map


# Libada Robot parameters
adaConf:
  end_effector: j2n6s200_forque_end_effector
  arm_controller: move_until_touch_topic_controller

# Robot parameters
ada:
  urdfUri: package://ada_description/robots_urdf/ada_with_camera_forque.urdf
  srdfUri: package://ada_description/robots_urdf/ada_with_camera_forque.srdf
  homeConfiguration: [3.89015, 3.34645, 2.28738, -1.39666, 2.96761, 2.49281]      # the start configuration of the robot joints
  baseFramePose: [0.7, -0.1, -0.25, 0, 0, 3.1415]                                 # the transform between the robot base and the left front corner of the table

# Workspace parameters
plate:
  urdfUri: package://pr_assets/data/objects/plate.urdf
  pose: [.445, 0.19, -0.045, 0, 0, 0]
table:
  urdfUri: package://pr_assets/data/furniture/table_feeding.urdf
  pose: [0.76, 0.38, -0.765, 0, 0, 0]
defaultFoodItem:
  urdfUri: package://pr_assets/data/objects/food_item.urdf
  pose: [.445, 0.19, -0.045, 3.1415, 0, 0]
person: # the mannequin
  urdfUri: package://pr_assets/data/objects/tom.urdf
  pose: [0.29, 0.28, 0.830, 0, 0, -1.57]
workspaceEnvironment:
  urdfUri: package://pr_assets/data/furniture/workspace_feeding_demo.urdf
  pose: [0, 0, 0.005, 0, 0, 0]
workspaceEnvironmentWithWallFurtherBack:
  urdfUri: package://pr_assets/data/furniture/workspace_feeding_demo.urdf
  pose: [0, 0, 0.005, 0, 0, 0]
wheelchair:
  urdfUri: package://pr_assets/data/furniture/wheelchair.urdf
  pose: [0.0, 0.0, 0.005, 0, 0, 0]

# Feeding demo movement parameters
feedingDemo:
  heightAbovePlate: 0.15
  heightAboveFood: 0.08
  heightIntoFood: 0.085
  moveOutofFood: 0.025
  #distanceToPerson: 0.25
  distanceToPerson: 0.05
  waitMillisecsAtFood: 00     # short pause after skewering the food
  waitMillisecsAtPerson: 1000   # long pause so person can take a bite
  fixedFaceY: 0.33

# Planning parameters
planning:
  maxNumberOfTrials: 1
  timeoutSeconds: 2
  batchSize: 100
  maxNumberOfBatches: 1
  numMaxIterations: 100
  tsr:
    horizontalToleranceAbovePlate: 0.01
    verticalToleranceAbovePlate: 0.03
    rotationToleranceAbovePlate: 3.1416
    horizontalToleranceNearFood: 0.001
    verticalToleranceNearFood: 0.008
    rotationToleranceNearFood: 0.01
    tiltToleranceNearFood: 0.00
    horizontalToleranceNearPerson: 0.005
    verticalToleranceNearPerson: 0.005
  endEffectorOffset:
    positionTolerance: 0.004
    angularTolerance: 0.004
  endEffectorTwist:
    positionTolerance: 0.0002
    angularTolerance: 0.004

# Parameters for the F/T sensor and the MoveUntilTouchController
ftSensor:
  controllerFTThresholdTopic: /move_until_touch_topic_controller/set_forcetorque_threshold/
  ftTopic: /forque/forqueSensor
  thresholds:
    standard:       # used when no other thresholds are specified
      force: 4
      torque: 4
    grabFood:       # active when going down to grab the food
      force: 22
      torque: 2
    afterGrabFood:  # active when going up after food has been grabbed
      force: 50
      torque: 2


# Perception parameters
perception:
  detectorDataUri: package://pr_assets/data/objects/tag_data_foods.json
  foodDetectorTopicName: /food_detector/marker_array
  faceDetectorTopicName: /face_detector/marker_array
  referenceFrameName: j2n6s200_link_base
  timeoutSeconds: 5
  faceName: mouth

foodItems:
  # names: ["apricot", "apple", "cantaloupe", "egg", "bell_pepper", "cherry_tomato", "banana", "carrot", "grape_green", "watermelon", "blackberry", "celery", "strawberry", "broccoli", "cauliflower"]
  # forces: [25,       11.75,     7.37,        4.67,  20.99,        11.36,            6.13,       22.49,      9.07,       5.21,          5.98,        17.48,      6.63,         21.63,    21.63]
  #names: ["cantaloupe", "egg",  "banana", "carrot", "celery", "strawberry", "melon"]
  #forces: [15,3,3,7,22,15,7]
  #pickUpAngleModes: [0, 2, 2, 0, 0, 0, 0]
  names: ["apple", "banana", "bell_pepper", "broccoli", "cantaloupe", "carrot", "cauliflower", "celery", "cherry_tomato", "grape", "honeydew", "kiwi", "strawberry", "lettuce", "spinach", "kale"]
  forces: [10, 10, 10, 10, 15, 22, 7, 22, 7, 10, 7, 7, 15, 7, 7, 7]
  pickUpAngleModes: [1, 5, 1, 2, 1, 1, 0, 1, 1, 3, 3, 3, 0, 1, 2, 0]

humanStudy:
  autoAcquisition: true
  autoTiming: true
  autoTransfer: true
  createError: false
  useAlexa: true
  foodTopic: /alexa_msgs
  actionTopic: /alexa_msgs

rotationFree:
  names: ["banana", "broccoli", "cantaloupe", "cauliflower", "cherry_tomato", "grape", "honeydew", "kiwi", "strawberry"]

tiltFood:
  names: ["apple", "bell_pepper", "celery"] # TODO:carrot should be added here once the tilt motion is fixed

study:
  skipSkewering: false
  doForkPickup: false

  tableHeight: 0.235

  # y positive is closer to wheelchair
  # z
  forkHolderTranslation: [0.558, -0.023, 0.037]
  forkHolderAngle: 0.0

  # 0 straight
  # 1 strawberry-style
  # 2 banana-style
  pickupAngleMode: 0

  # x: from operator
  # y: towards user 0.24
  # z: up

  # 0.73
  # TRUE:
  # personPose: [0.295, 0.24, 0.73, 0, 0, 3.1415]
  # ERRONEOUS:
  personPose: [0.295, 0.24, 0.73, 0, 0, 3.1415]
  tiltOffset: [0.0, 0.05, -0.02]

  # FAST: # NOTE: these parameters are not currently used
  velocityLimits: [0.7, 0.7, 0.7, 0.7, 0.7, 0.7]
  # SLOW:
  # velocityLimits: [0.2, 0.2, 0.2, 0.2, 0.2, 0.4]
