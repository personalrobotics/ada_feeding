### How to Run Plate Locator Algorithm
Open up some terminals in your computer (we assume you are using weebo that is connected to ADA robot in lab). Successfully build your workspace in each terminal by respectively doing: `cd <catkin_ws>; catkin build; . devel/setup.bash`. Run instrcutions 1-4 from [ada_feeding real robot demo](https://github.com/personalrobotics/ada_feeding/blob/main/README.md#demo-run-steps). Run the plate locator python script and behavior tree xml files with these commands respectively: `python src/ada_feeding/scripts/run_plate_detector.py` and `roslaunch ada_feeding feeding.launch treeFile:=plate.xml sim:=false use_forque:=false`.

Now, open up another terminal and go to the directory of [feeding web app](https://github.com/personalrobotics/feeding_web_interface/tree/2022_revamp/feedingwebapp). In the `2022_revamp` branch of the `feeding_web_interface` repo, go to the directory of `feedingwebapp`. From there run `npm start` which should start the app in your computer. If `npm` was not installed in your computer, run `npm install` before the `npm start` command. Also, please make sure to run `roslaunch rosbridge_server rosbridge_websocket.launch` from another terminal after building catkin workspace with above instuctions to start the webserver, so that `ros` and `feedingwebapp` can communicate through ros topics.

**Start App on Your Phone:** Connect your phone to the same wifi of your computer (in this case ADA_5G which is used by weebo). As the app is already running in weebo. Copy the url from the app tab's browser (something like `192.168.2.2:3000` which is weebo's IP address followed by port number) to browser in your phone. Then, the app should be running in your phone too. Go to the plate locator buttons by clicking on `Start Feeding` button from app's homepage.

### Different Components of Plate Locator Algorithm
1. The `run_plate_detector.py` script contains a ros subscriber and a ros service that respectively processes ros images from rviz camera topic to cv image. Then, the service uses that cv image to see if partial plate is detected, and if so, it computes movement direction for full plate detection.
2. The `plate.xml` runs the behavior tree that listens to ros msgs published by the app and moves the robot arm accordingly.
3. The `Home.js` file has all the ros topics and their publishers defined.